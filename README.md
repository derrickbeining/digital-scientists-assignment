# digital-scientists-assignment

Mobile Safari 11 and Mobile Chrome now support the MediaCapture and SpeechSynthesis APIs.  With that in mind:

* Build a simple mobile web app that posts the images captured on a phone's camera to the Google Cloud Vision API and reads out "new" objects detected.
* Try building the app in Chrome and then test it on iOS Safari / Android Chrome.
* Use GitHub to host the repo and share it once done.
* Host the app on a public url (Heroku/Github Pages) and send a link to the app as well. Be sure to tell us how much time was spent on the assignment.
* Do your best to be thoughtful about the experience of the application, not just the raw functionality of it.

## Helpful links:
* Speech synthesis: https://github.com/mdn/web-speech-api/tree/master/speak-easy-synthesis
* Mediacapture: https://davidwalsh.name/demo/camera.php
* Google Cloud Vision API: https://cloud.google.com/vision/
* ReactJS: https://reactjs.org/

## Hint:
Note that on iOS Safari, speech playback must be triggered by a user action (e.g. button press). This is presumably a feature rather than a bug.
http://ejb.github.io/2015/06/07/html5-speech-synthesis-api.html
